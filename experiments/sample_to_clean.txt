Journal of Artificial Intelligence Research 70 (2021) 1309-1334 Submitted 12/2020; published 04/2021
The AI Liability Puzzle and a Fund-Based Work-Around
Olivia J. Erd ́elyi
University of Canterbury, School of Law Christchurch 8140, New Zealand
Soul Machines
Auckland 1010, New Zealand
G ́abor Erd ́elyi
olivia.erdelyi@canterbury.ac.nz
gabor.erdelyi@canterbury.ac.nz
University of Canterbury, School of Mathematics and Statistics Christchurch 8140, New Zealand
Abstract
Confidence in the regulatory environment is crucial to enable responsible AI innovation and foster the social acceptance of these powerful new technologies. One notable source of uncertainty is, however, that the existing legal liability system is unable to assign re- sponsibility where a potentially harmful conduct and/or the harm itself are unforeseeable, yet some instantiations of AI and/or the harms they may trigger are not foreseeable in the legal sense. The unpredictability of how courts would handle such cases makes the risks involved in the investment and use of AI difficult to calculate with confidence, creating an environment that is not conducive to innovation and may deprive society of some bene- fits AI could provide. To tackle this problem, we propose to draw insights from financial regulatory best practices and establish a system of AI guarantee schemes. We envisage the system to form part of the broader market-structuring regulatory frameworks, with the primary function to provide a readily available, clear, and transparent funding mech- anism to compensate claims that are either extremely hard or impossible to realize via conventional litigation. We propose it to be at least partially industry-funded. Funding arrangements should depend on whether it would pursue other potential policy goals aimed more broadly at controlling the trajectory of AI innovation to increase economic and social welfare worldwide. Because of the global relevance of the issue, rather than focusing on any particular legal system, we trace relevant developments across multiple jurisdictions and engage in a high-level, comparative conceptual debate around the suitability of the foreseeability concept to limit legal liability. The paper also refrains from confronting the intricacies of the case law of specific jurisdictions for now and—recognizing the importance of this task—leaves this to further research in support of the legal system’s incremental adaptation to the novel challenges of present and future AI technologies.
1. Introduction
With proliferating AI-human interactions, issues around the civil and criminal liability for AI systems have moved to the forefront of legal policy debates. Can a bank using an AI-enabled lending decision-making system that unexpectedly turns out to unlawfully discriminate customers successfully sue the provider of the system? Who is liable if an autonomous vehicle (AV) hits a pedestrian or is involved in a crash? What happens if an AI is used in criminal actions owing to, say, an unexpected value alignment problem of the sort described in Schreier’s Robot and Frank or the canonical paperclip maximizer doomsday scenario?
©2021 AI Access Foundation. All rights reserved.
Erde ́lyi & Erde ́lyi
While each of these questions touches upon different domains of legal liability—contrac- tual, tort, and criminal liability, respectively—their core inquiry is the same: Who should be held accountable if something goes wrong with an AI and based on what rules? Well aware that courts and policymakers will soon have to come up with satisfactory answers, a growing number of papers has taken a first crack at examining the topic from various perspectives. The result is a landscape of conflicting accounts on how best to go about AI liability and the legal system’s overall ability to adapt to this latest wave of technological innovation.
Some further the debate by synthesizing the relevant literature on selected aspects of civil and criminal liability (Kingston, 2016; Perc, Ozer, & Hojnik, 2019). Among those having faith in the existing system’s adequacy to deal with AI liability issues is Hubbard (2016), who—tacitly invoking the famous Hand formula (Judge Learned Hand, 1947)—concludes that the current US system of contractual and tort liability strikes a fair and efficient balance between ensuring safety and incentivizing innovation. Consequently, they see no reason to apply different metrics to compensating physical injury inflicted by sophisticated robots (which they define as having some degree of connectivity, autonomy, and potentially machine-learning (ML) ability).
Given the apparent imminence of the topic, quite a few papers revolve around AVs. Liechtung (2018) urges for timely adjustment of regulation and oversight mechanisms to prepare for the impending mass-release of AVs. They also stress the importance of clarity and predictability of the legal liability regime—whatever liability rules are chosen—so those involved in the development, production, and distribution of AVs can better assess their risk exposure.
Several commentators argue for subjecting AVs or AI more broadly to strict liability— commonly some type of products liability regime (Gerstner, 1993; Liechtung, 2018). An interesting recent idea in this realm has been put forward by Vladeck (2014), advocat- ing a strict liability regime entirely detached from notions of fault—in essence a court- implemented insurance system. In a practical, goal-oriented, if slightly doctrinally incon- sistent approach, they propose to simply infer liability from negative outcomes to overcome situations where it is impossible to establish fault. Doing so, they hope to create a more cost- efficient, equitable, and predictable liability regime, which provides a safe and stimulating environment for innovation, better protection to blameless parties, and fairer cost-spreading among affected parties. As a way of achieving the latter, they contemplate abandoning the current practice of treating AV liability as an agency question (K ̈ohler, 2018) and confer- ring legal personality on AVs coupled with a compulsory self-insurance instead. Relatedly, Karnow (1994) advocates an electronic personality for autonomous robots (by which they mean those with an ML component) to enable the legal system to hold them directly liable under tort law. Whether making AI systems legal persons—thereby eliminating any direct human responsibility and oversight of their operation and impact on the environment—is a sound idea is a controversial issue in itself, with numerous ethical, design, and legal consid- erations speaking against it (Institute of Electrical and Electronics Engineers, 2019; Bryson, Diamantis, & Grant, 2017).
A contrasting view reveals concerns about potentially ludicrous expenses involved with complex products liability suits, pre-trial settlements, product recalls, and punitive dam- ages, pressing for a meticulous application of the negligence doctrine to AV incidents (Green-
1310

The AI Liability Puzzle and A Fund-Based Work-Around
blatt, 2016). They maintain that equal treatment of AVs and those under human guidance in this manner would also result in a higher degree of legal certainty spurring innovation—a common theme supporting all the above views—and allow for the operation of market-based incentives such as reputational concerns.
Finally, Karnow (2016) expresses doubts as to whether any of the classic United States tort doctrines—negligence and the various forms of strict liability—is up to allocating lia- bility for wrongdoings of truly autonomous robots. This is because foreseeability is a central element of each of these doctrines, however, due to complex non-linear interactions between intricate robots and their equally convoluted, unpredictable environment, neither robots’ actions nor the potential harms they may cause are necessarily foreseeable in the sense re- quired by law. Regarding AVs and autonomous robots and mostly in the context of United States tort law, other commentators have voiced similar concerns about potential liability gaps and the implications of a resulting overall uncertainty surrounding the legal liability of AI systems. They confirm Karnow’s observation about the centrality of foreseeability in limiting legal liability but concede that emergent behavior—i.e., behavior contingent on the interaction of a system’s elements rather than the elements themselves—exhibited by some systems may trigger genuinely unforeseeable categories of harm (Barfield, 2018; Calo, 2018). This unpredictability of foreseeability makes it even harder to evaluate the chances of success of litigation and hence exposure to liability, adding to the uncertainties that flow from the inconsistency of jurisprudence during the typically significant time lag needed for the legal system to adapt to novel technologies. As explored in economics and law and technology literature, the presence of uncertainty—especially coupled with the absence of individuals’ ability to insure themselves against it—can significantly inhibit innovation and the adoption of new technologies, in extreme cases reaching as far as shutting down entire emerging markets (Arrow, 1962; Pearl, 2018).
In this paper, we restrict the focus of the above sketched AI liability debate by analyzing only the foreseeability concept’s ability to serve as a means to limit and attribute legal liability. At the same time, however, we will also move this important discussion beyond United States tort law and embodied AI systems or particular AI applications—indeed beyond any national analysis and law in general, for the following reasons:
AI is just one of the most recent waves of technological innovation (sometimes referred to as the fourth industrial revolution) all of which have fundamentally impacted our societies and economies. Due to its rapid pace of development, massively transformative nature, and other changes—most notably globalization—our world has undergone, AI is antici- pated to affect humanity and our environment even more intensely. Recognizing this, there are major national, regional and international AI strategies and policy initiatives under- way, which aim to forge an innovation-friendly, enabling regulatory environment, capturing benefits and minimizing potential risks AI may bring (G20, 2019; Organisation for Eco- nomic Co-operation and Development (OECD), 2019; European Commission High-Level Expert Group on Artificial Intelligence, 2019; European Commission, 2020c; Abrahams et al., 2019). All these initiatives converge on the point that successful societal adoption of AI—like any other form of technological innovation—requires trust on the part of society. Trust, in turn, hinges on at least some level of certainty about how AI will impact society and the economy: Developers need to be able to assess the risks inherent in bringing a new product on the market, while consumers and other users of the technology must be assured
1311

Erde ́lyi & Erde ́lyi
that its use is reasonably safe. Without such trust and certainty, innovation in emerging technologies is likely to be severely stifled, hampering economic growth and welfare (Arrow, 1962; Pearl, 2018). Certainty itself flows from a safe, transparent, and flexible regulatory environment that supports innovation. The 2018 fatal Uber AV crash in Tempe, Arizona, and its aftermath, on the other hand, provides a warning example of how regulatory fail- ures coupled with human errors can shatter trust and threaten markets in an emerging technology (National Transportation Safety Board (NTSB), 2018; Templeton, 2019). Yet, as explained below, designing any aspect of the nascent AI regulatory framework—such as an adequate legal liability regime—is neither a purely legal nor an exclusively national enterprise.
From an economic perspective, the regulatory frameworks that structure our economies together with market imperfections crucially determine the extent to which society ben- efits from technological innovation (Stiglitz, 2015). It is established wisdom that in our reality of imperfect markets, technological innovation is not necessarily Pareto-improving. On the contrary, in the absence of cleverly devised and potentially substantial redistribu- tive measures, it can actually aggravate inequality and decrease overall welfare (Korinek & Stiglitz, 2019). Stiglitz (2015) also shows that inequality-related problems can only be effectively tackled by a holistic approach involving a complete and systematic revamp of market-structuring regulatory frameworks, which legal liability regimes are admittedly part of. This argument advocating a holistic regulatory stance also holds true more generally when it comes to optimally aligning different regulatory objectives within a broader regu- latory system.
The above cited key international AI policy documents, and—based on a review of rele- vant international relations literature— Erd ́elyi and Goldsmith (2018, 2020) also underscore the necessity of international coordination and cooperation in the AI domain. The core of the arguments here is that issue areas with transnational impact, such as AI, can be far more effectively regulated in an internationally coordinated manner—be it in the form of truly transnational policy initiatives or national measures that display at least some de- gree of coordination. The reason is that without coordination, domestic approaches will inevitably be fragmented. This not only invokes inefficiencies and tensions in international policymaking, but also negatively affects domestic regimes, shattering both national and international actors’ faith in the viability of national regulatory approaches.
These arguments combined call for a holistic, multidisciplinary, and transnational per- spective, forbidding an isolated legal or nationally focused analysis of liability regimes. Hence, Section 2 will start with a high-level, comparative legal analysis, which should help the reader to navigate the differences in terminology and/or legal approaches that persist between the two main legal traditions—common law and civil law systems—and even across countries belonging to the same tradition. It will explain why the notion of foreseeability is central to attributing legal liability in all legal systems and highlight a potential conceptual problem related to its suitability to constrain the legal liability of AI systems. We argue that foreseeability is common to all types of legal liability irrespective of the area of law they originate from, and may raise attribution problems in relation to the actions of any embodied or disembodied AI system, provided it uses certain types of ML models. Note that although under the current state of technology these problems arise in connection with certain (not all) ML-based systems, in the future they are equally conceivable in relation
1312
INFORMATION & COMMUNICATIONS TECHNOLOGY LAW 2021, VOL. 30, NO. 2, 208–234 https://doi.org/10.1080/13600834.2020.1861714
Property ownership and the legal personhood of artificial intelligence
Rafael Dean Brown
Centre for Law and Development, Qatar University College of Law, Doha, Qatar
  
ABSTRACT
This paper adds to the discussion on the legal personhood of artificial intelligence by focusing on one area not covered by previous works on the subject – ownership of property. The author discusses the nexus between property ownership and legal personhood. The paper explains the prevailing misconceptions about the requirements of rights or duties in legal personhood, and discusses the potential for conferring rights or imposing obligations on weak and strong AI. While scholars have discussed AI owning real property and copyright, there has been limited discussion on the nexus of AI property ownership and legal personhood. The paper discusses the right to own property and the obligations of property ownership in nonhumans, and applying it to AI. The paper concludes that the law may grant property ownership and legal personhood to weak AI, but not to strong AI.
 1. Introduction
Human development, when viewed within the larger context of the earth’s existence, has been a very recent phenomenon. If one were to compare the earth’s actual existence as lasting a year, human development1 in comparison has only been for approximately one minute. Artifi- cial intelligence (AI), in turn, has not even registered a full one second.
AI is still in its infancy. Yet, the rapid advances made in the field of AI has already been astonishing. AI awed the world in 1997 when IBM’s Deep Blue supercomputer beat then reigning chess world champion Garry Kasparov. 2 In March 2016, Google’s AI computer Alpha Go, developed by Google’s DeepMind, defeated world Go champion Lee Sedol.3 Go is considered a more complex and challenging board game than chess because playing the game requires anthropomorphic intuition and pattern recognition.4 Pre- viously in 2011, IBM’s Watson, a cognitive supercomputer, defeated former Jeopardy!
CONTACT Rafael Dean Brown rbrown@qu.edu.qa
1There have been proposals to call this era the Age of Man or ‘Anthropocene’.
2IBM, ‘Icons in Progress: Deep Blue’ (2020) <http://www-03.ibm.com/ibm/history/ibm100/us/en/icons/deepblue/>
accessed 5 December 2020. Kasparov lost previously in 1996.
3D Muoio, ‘Why Go is So Much Harder for AI to Beat Than Chess’ Business Insider (New York, 10 March 2016) <https://
www.businessinsider.com/why-google-ai-game-go-is-harder-than-chess-2016-3 > accessed 5 December 2020. 4ibid.
© 2020 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group
This is an Open Access article distributed under the terms of the Creative Commons Attribution-NonCommercial-NoDerivatives License (http://creativecommons.org/licenses/by-nc-nd/4.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited, and is not altered, transformed, or built upon in any way.
KEYWORDS
Artificial intelligence; AI; legal personhood; personality;
property ownership; moral theory
 
INFORMATION & COMMUNICATIONS TECHNOLOGY LAW 209
champions Brad Rutter and Ken Jennings.5 Watson was a significant advancement from Deep Blue because its software could process and reason with natural language.6
However profoundly impressive were AI’s victories in the realm of games, society will ultimately measure AI’s success or usefulness by the advancement of its application beyond gaming. Aside from Google’s renowned driverless car, AI has recently demon- strated anthropomorphic learning and decision-making in two coveted professions: law and medicine.
In October 2017, partners and associates from top international law firms competed with Case Crunch, an AI start up in the UK, to assess and predict the success of almost 800 historic insurance misselling claims.7 Case Crunch handily won with 87 percent accu- racy against the lawyers’ 62 percent accuracy.8 In May 2016, the law firm of Baker and Hos- tetler announced that it had employed Ross, dubbed as the world’s first ‘artificially intelligent attorney’, to assist in its bankruptcy practice.9 Ross is built on IBM’s Watson, the same computer system that had beaten the Jeopardy! champions five years earlier.10 Ross, in addition to being able to process and reason with natural language, can now postulate hypotheses from questions asked, conduct legal research, and gener- ate response with references and citations, as a lawyer would.11 Most importantly, Ross is now self-learning, getting better and faster from experience and interaction.12 Around the same time in May 2016, the Children’s National Medical Center in Washington demon- strated that a supervised autonomous robot called Smart Tissue Autonomous Robot (STAR) could perform more superior soft tissue surgery than human surgeons could.13 While robots have previously assisted in surgery,14 surgeons consider soft tissue surgery to be more challenging for robots because of tissue deformity and mobility.15
At the current rate, AI resembling adult human intelligence may be achieved within 50 years.16 Along each step, both critics and enthusiasts must raise ethical, philosophical, and legal questions posed by the rise of AI. This paper focuses on a specific legal issue within the larger AI discussion: whether the legal personhood of AI ought to be recognised. Society, as a practical matter, will likely be forced to confront the legal issues raised by AI. In fact, legal issues are already being raised concerning autonomous machines. What happens when an autonomous vehicle gets into an accident, when the AI attorney is sued for malpractice, or the patient of an AI surgeon dies?
5IBM (n 2); J Watson, ‘Jeopardy, and Me, the Obsolete Know-It-All’ (2016) TEDxSeattleU <https://www.ted.com/talks/ken_ jennings_watson_jeopardy_and_me_the_obsolete_know_it_all?language=en>.
6IBM (n 2).
7C Johnson, ‘Artificial Intelligence Beats Big Law Partner in Legal Matchup’ The American Lawyer (32 October 2017)
<https://www.law.com/americanlawyer/sites/americanlawyer/2017/10/31/artificial-intelligence-beats-big-law-
partners-in-legal-matchup/?slreturn=20201105095653> accessed 5 December 2020. 8ibid.
9C De Jesus, ‘Artificially Intelligent Lawyer ‘Ross’ Has Been Hired By Its First Official Law Firm’ (Futurism, 11 May 2016) <http://futurism.com/artificially-intelligent-lawyer-ross-hired-first-official-law-firm/> accessed 5 December 2020.
10ibid.
11ibid.
12ibid.
13A Shademan and others, ‘Supervised Autonomous Robotic Soft Tissue Surgery’ (2016) 8 STM 337; M Senthilingam
‘Would You Let a Robot Perform Surgery by Itself?’ CNN (12 May 2016) <http://edition.cnn.com/2016/05/12/health/
robot-surgeon-bowel-operation/index.html> accessed 5 December 2020.
14An example is the da Vinci Surgical System.
15Shademan (n 13).
16With such fast development, the field of AI has already driven some to ponder the gravity of what it means to achieve
 strong AI, or AI that goes beyond human intelligence. Tempting as it may be, this author will avoid such discussions.

210 R. D. BROWN
The issue of whether an AI, or an autonomous system for that matter, will be held legally accountable ought to begin with the question on who owes a legal duty for the negligent or criminal acts of an AI: the AI, the algorithm programmer, or its inventor/ owner. Can the AI be sued? The inverse to this question is whether the AI has the right to sue.17 In the U.S., the issue of right to sue may also raise the related constitutional issue of standing.
The first question deals with the imposition of a legal duty on AI, while the second question deals with the granting of legal rights on AI. As discussed further below, impos- ing a legal duty only on the owner of the AI causes obstacles to legal accountability in instances where the owner has limited knowledge and control over the AI’s conduct, a predicament that can also raise obstacles in establishing causation. A proposed solution is to grant legal person status to AI and similar computer systems.
This paper aims to add to the discussion on the proposed legal personhood of AI by focusing on one area that previous works on the subject have avoided – the relationship between legal personhood and ownership of property.18 Solum and other scholars that followed,19 for example, opted for a discussion on the legal personhood of AI that relies on insurance as the source for the collectability of a legally recognised AI entity.20
Upon closer examination of the concept of legal personhood, however, it becomes apparent that the essence of legal personhood rests on the right to own property. This paper will discuss how the right to own property leads to the argument in favour of legal personhood for weak AI, but not for strong AI. The right to own property is a pre- requisite for legal personhood for one very practical reason: patrimony or collectability.21 Yet, while scholars like Rothenberg and Denicola have begun to write about the concept of AI owning real property22 and copyright,23 respectively, there has been an absence of scholarship on the nexus of AI property ownership and AI legal personhood.24
Part II begins by defining AI, and the need for AI legal personhood. Part III discusses the literature on legal personhood, and argues that AI can theoretically attain legal person- hood even without a will. Part III compares the persona ficta and juristic person approaches to legal personhood, and discusses the concept of will in rights or duties. Part III then applies the requirement of rights or duties to AI.
Part IV discusses the nexus between property ownership and AI legal personhood. The section first discusses the interconnected concepts of property ownership and legal personality before discussing the right to own property and the obligations of property ownership in nonhumans. The section then applies the rights and duties of property own- ership to AI, and argues that the right to own property could be conferred to weak AI, but not to strong AI, because of the necessity of establishing a will, which needs to be attrib- uted from a human agent. The section also discusses the imposition on nonhumans and
17In the U.S., the issue of right to sue may also raise the related constitutional issue of standing.
18S Chopra and L White, ‘Artificial Agents – Personhood in Law and Philosophy’ (ECAI, 2004) <http://www.sci.brooklyn.
cuny.edu/~schopra/agentlawsub.pdf> accessed 5 December 2020.
19L Solum, ‘Legal Personhood for Artificial Intelligence’ [1992] 70(4) North Carolina L Rev 1231, 1245.
20Chopra (n 18).
21ibid.
22D Rothenberg, ‘Can Siri 100 Buy Your Home? The Legal and Policy Based Implications of Artificial Intelligent Robots
Owning Real Property’ [2016] 11(5) Washington Journal of Law, Technology & Arts 439.
23R Denicola, ‘Ex Machina: Copyright Protection for Computer-Generated Works’ [2016] 69 Rutgers L Rev 251. 24Rothenberg (n 22) 439; Denicola (n 23) 251.
 
INFORMATION & COMMUNICATIONS TECHNOLOGY LAW 211
AI in particular of the obligations of property ownership, and argues that the law could impose and enforce the obligations on weak AI, but not on strong AI. Finally, part IV dis- cusses whether AI should own property, taking into account the moral theory of property, and how AI can own property. The paper concludes that legal personhood is the best approach for AI to own personal property.
2. Defining AI and the need for legal personhood
John McCarthy first coined the term ‘artificial intelligence’ in a proposal for a Dartmouth summer conference in 1956.25 Whether strong AI is possible remains an ongoing debate. Yet, news of an advanced machine-learning computer defeating humans in various fields including law, continue to push the discussion closer to the possibility of a strong AI. To be clear, what most refer to as AI today is not AI as defined under the most widely recog- nised test for AI – the Turing test. Alan Turing, the same man who helped win the Second World War, is most known today for a test he devised for determining whether a machine can think. The Turing test has since remained at the centre of the debate regarding AI. According to the Turing test, a machine achieves artificial intelligence if it can convince a questioner that the machine is human, half as often as a human can.
There has yet been no proven claim of a machine or software that has met the Turing test.26 The type of AI that would likely meet the Turing test is that with an intellectual capability that is equal to a human, called ‘strong AI’, which has intentionality and con- sciousness.27 In law, this is the equivalent of having a will. Computer scientists, however, claim that we are closer than ever at achieving strong AI. Despite the astonish- ing feats of AI in defeating humans at advanced strategy games, conducting medical surgery, flying a fighter jet simulation, and conducting legal research and analysis; what some mistakenly call AI today is actually machine learning, or ‘weak AI’. Weak AI is not equal to human intelligence but relies on humans to engage tasks while enhancing performance time and accuracy.28 Machine learning is only but one of a number of fields under AI.29
Despite the popularity of the Turing test, there remains no widely agreed upon definition of AI.30 It is widely recognised that AI is difficult to define.31 According to Scherer, the difficulty in defining AI stems not in defining what is ‘artificial’, but rather in defining the term ‘intelligence’.32 Existing definitions of intelligence are tied to human intelligence. As stated by John McCarthy, there is no ‘solid definition of
25G Press, ‘Artificial Intelligence (AI) Defined’ Forbes (2017) <https://www.forbes.com/sites/gilpress/2017/08/27/artificial- intelligence-ai-defined/?sh=56189acf7661> accessed 5 December 2020; J Truby, R Brown, and A Dahdal, ‘Banking on AI: Mandating a Proactive Approach to AI Regulation in the Financial Sector’ [2020] 14(2) Law and Financial Markets Review 110–20.
26On the same token, there has been no serious attempts to try to prove the Turing test.
27J Searle, ‘Minds, Brains, and Programs’ [1980] 3(3) Behavioral and Brain Sciences 417–57.
28ibid.
29Truby (n 25); J Newman, ‘Toward AI Security: Global Aspirations for a More Resilient Future’ (CLTC White Paper Series,
2019) <https://cltc.berkeley.edu/wp-content/uploads/2019/02/CLTC_Cussins_Toward_AI_Security.pdf> accessed 16
May 2020.
30M Scherer, ‘Regulating Artificial Intelligence Systems: Risks, Challenges, Competencies, and Strategies’ [2015] 29(353)
Harvard Journal of Law and Technology 3–4. 31Truby (n 25).
32Scherer (n 30).
 
212 R. D. BROWN
intelligence that doesn’t depend on relating it to human intelligence’.33 Another definition of AI is ‘a suite of autonomous self-learning and adaptively predictive technol- ogies that enhances the ability to perform tasks’.34
AI may also be categorised into four types: (1) systems that think like humans, (2) systems that act like humans, (3) systems that think rationally, and (4) systems that act rationally.35 The Turing test falls under the systems that think like humans category. Instead of the Turing test, a simple way of conceptualising AI is to think of it simply as software or programmed code.36
This paper does not aim to offer a novel scientific definition of AI. Rather, this paper focuses on a working definition of AI for purposes of legal application. In this way, lawyers ought to consider proposing a legal (rather than a scientific or philosophical) definition for AI. Such a legal definition ought to be tied to AI’s status and functions in legal relations and transactions, whether as an artificial agent or as a technological tool for efficient contract formation, as in smart contracts. For purposes of this paper, the author proposes that a legal definition for AI should include the concept of rights and duties that are necessary for legal personality. In law, the essence of asserting a set of rights, or duties, is tied to the ownership of property, as discussed further below. A legal definition of AI, for example, could be an autonomous self-learning and adaptively predictive technology consisting of codes that can think or act in order to exercise legal rights or perform duties that are incident to property ownership.
2.1. The need for the legal personhood of AI
That the law does not grant legal personhood to AI creates legal obstacles that lead to uncertainties.37 This is especially so as AI becomes more autonomous, making the appli- cation of legal rules involving AI more challenging.38 For example, concerns have been
33J McCarthy, ‘What is Artificial Intelligence?’ (Computer Science Dept, Stanford 2017) <http://jmc.stanford.edu/articles/ whatisai/whatisai.pdf> accessed 5 December 2020.
34Truby (n 25). See also European Parliament (EP), ‘Recommendations to the Commission on a Civil liability regime for artificial intelligence’ (Draft Report) CLA 2020/2014(INL), 27 April 2020 <https://www.europarl.europa.eu/doceo/ document/JURI-PR-650556_EN.pdf> accessed 27 November 2020 [EP 2020] (defining ‘AI system’ as ‘a system that dis- plays intelligent behaviour by analysing certain input and taking action, with some degree of autonomy, to achieve specific goals. AI systems can be purely software-based, acting in the virtual world, or can be embedded in hardware devices’).
35S Russel and P Norvig, Artificial Intelligence: A Modern Approach (3rd edn, Pearson 2010) 5.
36Truby (n 25).
37EP 2020 (n 34). The Committee on Legal Affairs in its Draft Report to the Commission on a Civil liability regime for artifi-
cial intelligence, clarifies that ‘AI-systems have neither legal personality nor human conscience, and that their sole task
is to serve humanity’.
38The Committee on Legal affairs of the European Parliament also points to autonomy as potentially triggering the need
for AI legal personhood when it put forward the following language in its Motion for a European Parliament Resolution in respect of robotics and artificial intelligence, which was later adopted as the Civil Law Rules on Robotics: ‘[T]he more autonomous robots are, the less they can be considered simple tools in the hands of other actors (such as the man- ufacturer, the owner, the user, etc.); ... this, in turn, makes the ordinary rules on liability insufficient and calls for new rules which focus on how a machine can be held—partly or entirely—responsible for its acts or omissions; ... as a con- sequence, it becomes more and more urgent to address the fundamental question of whether robots should possess a legal status’. European Parliament (EP) ‘Motion for a European Parliament Resolution’ CLA 2015/2103(INL), 27 January 2017 <https://www.europarl.europa.eu/doceo/document/A-8-2017-0005_EN.html> accessed 30 November 2020. The Committee also called on the Commission for Civil Law Rules and Robotics to conduct ‘an impact assessment of its future legislative instrument, to explore, analyse and consider the implications of all possible legal solutions’ in ‘creating a specific legal status for robots in the long run, so that at least the most sophisticated autonomous robots could be established as having the status of electronic persons responsible for making good any damage they may cause, and possibly applying electronic personality to cases where robots make autonomous decisions or otherwise interact with third parties independently’. ibid.
 
INFORMATION & COMMUNICATIONS TECHNOLOGY LAW 213
raised concerning the allocation of liability,39 copyright ownership in works indepen- dently created by AI,40 digital clones,41 and contracting with artificial agents,42 among others. Because a discussion of all the various legal uncertainties created by AI without having legal personhood is beyond the scope of this paper, it will only focus on the more prevalent issue of contracting involving AI.
Uncertainties created with contracts involving AI have raised the issue of the need to grant legal personhood to AI.43 Bidding sites like eBay, for example, allow a user to rely on ‘shopbots’ or ‘pricebots’ to automatically bid on items sold on the website.44 Legal doc- trinal challenges to the contract arise on whether the parties were aware of the terms and whether the artificial agent has the intent to enter into the contract.45 Nuanced difficulties from the same legal obstacle to contracting could later arise with AI agents that have increased autonomy and could speak, write, or even act like a human.
Critics of granting legal personhood argue that alternative legal doctrines to contract formation would be sufficient. Such alternatives include treating AI as mere tools, treating AI as an artificial agent governed by agency law, and using insurance to cover potential AI liability. Currently, with less advanced artificial agents, the obstacle to electronic contract- ing was resolved by resorting to analysis that classify artificial agents as ‘tools’ of humans,46 treat the contract as a unilateral offer by the artificial agent,47 or apply the objective theory of contract law.48 Such alternatives will reach their limit, however, as AI, including artificial agents, engage in increasingly autonomous, unforeseeable, and uncontrolled actions and decision-making.
Increased autonomy diminishes the argument that humans use the artificial agent as a mere tool, or that the artificial agents intended a unilateral contract.49 Increased auton- omy also increases the artificial agent’s ability to make a variety of choices and intentions.
39ibid. Even the EU’s strict liability approach to AI grappled with the challenge of human programmers, controllers, deployers, or owners of AI escaping liability due to the act of an AI that is not their fault. The Committee on Legal Affairs in its Draft Report provides an exception from liability when the deployer can prove that the harm or damage was caused without his or her fault since
‘(a) the AI-system was activated without his or her knowledge while all reasonable and necessary measures to avoid such activation were taken, or (b) due diligence was observed by selecting a suitable AI-system for the right task and skills, putting the AI-system duly into operation, monitoring the activities and maintaining the operational reliability by regularly installing all available updates.’ EP 2020 (n 34) art 8(2)
40See generally, P Devarapalli, ‘Machine Learning to Machine Owning: Redefining the Copyright Ownership from the per- spective of Australian, US, UK and EU Law’ [2018] 40(11) European Intellectual Property Review 722–28 (concluding that almost all jurisdictions requires a ‘person’ or ‘human’ to be an author or owner of a creative work to be copyright pro- tected, and therefore works directly created by an AI or with the assistance of an AI has no copyright protection with varying degrees of approaches among jurisdictions as to the ownership of the work).
41J Truby and R Brown, ‘Human Digital Thought Clones: The Holy Grail of Artificial Intelligence for Big Data’ [2020] Infor- mation and Communications Technology Law. doi:10.1080/13600834.2020.1850174 <https://www.tandfonline.com/ doi/full/10.1080/13600834.2020.1850174> accessed 5 December 2020 (posing the question of ‘whether digital clones can retain or attain the status of personhood in the legal or philosophical sense’).
42Chopra (n 18).
43T Allan and R Widdison, ‘Can Computers Make Contracts?’ [1996] 9 Harvard Journal of Law and Technology 25–52; I
Kerr, ‘Ensuring the Success of Contract Formation in Agent Mediated Electronic Commerce’ [2001] 1(1/2) Electronic
Commerce Research 183–202. 44Chopra (n 18).
45ibid.
46Kerr (n 43) 183–202.
47I Kerr, ‘Providing for Autonomous Electronic Devices in the Uniform Electronic Commerce Act 1999’ (ULCC, 1999) <https://www.ulcc.ca/en/annual-meetings/359-1999-winnipeg-mb/civil-section-documents/362-providing-for- autonomous-electronic-devices-in-the-electronic-commerce-act-1999> accessed 5 December 2020.
48Chopra (n 18). 49ibid.
 
214 R. D. BROWN
Increased autonomy, therefore, also increases the unpredictability of artificial agents, and such unpredictability would pose a challenge to both the unilateral offer and the objec- tive theory approaches.50
Another potential alternative to the contracting problem is to treat AI as an agent. However, treating AI as a legal agent requires that the AI must have capacity to give legal consent, have the ability to exchange promises, and in civil law systems, be a person with sufficient mental capacity. In other words, the agency theory would in essence still require legal personhood.51 Legal personhood, therefore, will increasingly become a necessity as artificial agents evolve into highly autonomous AI.
An alternative approach to property ownership of AI is the one proposed by Solum: the use of insurance to cover the liability of an AI deemed as a legal person. This approach, however, opens a number of legal issues related to insurance law, and raises the same issues posed by agency and contract law. It is a temporary solution. For the same reasons that corporate liability is not solved by simply insuring the corporation, AI liability will not be solved by simply insuring the AI. One reason is that AI may actually control and possess property, and owners of AI may be able to avoid liability by simply having the AI control and possess property. The insurance industry will not likely allow itself to bear the risk, where the insurance company could spend substantially for the liability of an AI. One can imagine an AI, for example, that has manipulated the digital currency market or initial coin offering (ICO), costing billions of dollars in losses. Insurance would simply not cope with the risk, and could not cover all types of AI liability. Insurance companies will likely limit coverage to risks that they foresee, bringing the inquiry back to the issue of foresee- ability and the autonomy of the machine. Additionally, the insurance approach prevents the legal system from imposing punitive and restraining orders on the AI.
Another uncertainty created by the use of AI in contracting is that of contractual or judicial enforcement. Chopra and White noted that civil law countries require that assets be under the control of a legal person in order to satisfy a judgment.52 In other words, assets that are not under the control of a legal person would not be subject to judgment and enforcement, a problem that could be exacerbated with an AI that controls and possesses property.
Title: Granting legal personhood to artificial intelligence systems and traditional veil-piercing concepts to impose liability.
Ben Chester Cheong
Lecturer of Law
Singapore University of Social Sciences
Abstract: This article discusses some of the issues surrounding artificial intelligence systems and whether artificial intelligence systems should be granted legal personhood. The first part of the article discusses whether current artificial intelligence systems should be granted rights and obligations, akin to a legal person. The second part of the article deals with imposing liability on artificial intelligence beings by analogising with incorporation and veil piercing principles in company law. It examines this by considering that a future board may be replaced entirely by an artificial intelligence director managing the company. It also explores the possibility of disregarding the corporate veil to ascribe liability on such an artificial intelligence beings and the ramifications of such an approach in the areas of fraud and crime.
Introduction
The term “artificial intelligence” has been used loosely to describe a wide range of technologies, including “natural language interfaces to the software used for self-driving cars”.1 The article considers predominantly the “strong” conception of artificial intelligence.2 This form of artificial intelligence suggests that an artificial intelligence system of the future would be able to completely replace a human actor.3 There are a few manifestations of this and a possibility may be that a humanoid robot would be totally indistinguishable from humans.4 Upgrade, a 2018 movie directed by Leigh Whannell has demonstrated how this possibility might arise. In that movie, an artificial intelligence chip was implanted into a quadriplegic following a sinister accident to enable him to walk again. The chip eventually assumed control of that individual by psychologically breaking that individual’s mind, allowing the artificial intelligence system within the chip to assume total control over that individual’s mind and body.5
Researchers have deployed various permutations of the Turing Test to assess the depth of a system’s ability to exhibit intelligent behaviour indistinguishable from that of a human.6 Consider the movie’s
1 See for instance Martin Cunneen, et al., “Autonomous Vehicles and Embedded Artificial Intelligence: The Challenges of Framing Machine Driving Decisions” (2019) 33(8) Applied Artificial Intelligence 706-731.
2 Lance Eliot, “Strong AI Versus Weak AI Is Completely Misunderstood, Including For AI Self-Driving Cars” Forbes (15 July 2020) <https://www.forbes.com/sites/lanceeliot/2020/07/15/strong-ai-versus-weak-ai-is-completely- misunderstood-including-for-ai-self-driving-cars/#143b1a93227c> (accessed 20 October 2020).
3 Vincent C Müller, “Ethics of Artificial Intelligence and Robotics” (Winter 2020 Edition) The Stanford Encyclopedia of Philosophy in Edward N. Zalta (ed.) <https://plato.stanford.edu/archives/win2020/entries/ethics-ai/> (accessed 26 January 2021).
4 Carl Strathearn, “Our Turing Test for androids will judge how lifelike humanoid robots can be” The Conversation (30 July 2019) <http://theconversation.com/our-turing-test-for-androids-will-judge-how-lifelike-humanoid- robots-can-be-120696> (accessed 13 November 2020).
5 Connie Guglielmo, “Horror mastermind Leigh Whannell plays out our AI fears in Upgrade” (31 May 2018) CNET <https://www.cnet.com/news/horror-director-leigh-whannell-plays-out-our-ai-fears-in-upgrade-saw- insidious/> (accessed 26 January 2021).
6 Carl Strathearn, “Our Turing Test for androids will judge how lifelike humanoid robots can be” The Conversation (30 July 2019) <http://theconversation.com/our-turing-test-for-androids-will-judge-how-lifelike-humanoid- robots-can-be-120696> (accessed 13 November 2020).
   Electronic copy available at: https://ssrn.com/abstract=3857504
situation where an artificial intelligence chip is implanted into a human being. What if a hacker removes the system’s input guards, thereby causing the system to function beyond its prescribed “safe” limits? If an artificial intelligence system exhibits intelligent behaviour indistinguishable from humans, such that one does not know whether acts or omission committed by that individual can be attributed to the artificial intelligence chip or the human actor, then how should we ascribe liability if the artificial intelligence system results in that individual carrying out wrongful acts or omissions beyond that individual’s control? The first part of the article discusses whether current artificial intelligence systems should be granted rights and obligations, akin to a legal person. The second part of the article deals with imposing liability on artificial intelligence beings by analogising with incorporation and veil piercing principles in company law. It examines this by considering that a future board may be replaced entirely with an “artificial intelligence director”7 managing the company. It also explores the possibility of disregarding the corporate veil to ascribe liability on such an artificial intelligence beings and the ramifications of such an approach in the areas of fraud and crime.
The weak form of artificial intelligence
As described by a commentator, the current artificial intelligence in its current form is a type of “narrow AI or weak AI which does one specific task better than humans but fails in all other tasks due to lack of conscience and human emotions and intelligence”.8
This notion of consciousness sets apart a natural person from a machine. Two prerequisites for consciousness is the feeling of arousal and awareness. The part of the brainstem that allows one to retain consciousness is the rostral dorsolateral pontine tegmentum (at least in a study conducted by Harvard researchers).9 If this view is adopted, the difference between an artificial intelligence system and a human being is the concept of “consciousness”.
Looking at one common law jurisdiction, Singapore, BOM v BOK10 was a case which involved a wife who was formerly a practising lawyer who, together with her father, sought to deprive her husband of almost all his assets in the aftermath of an extremely traumatic event in the husband’s life by getting him to sign a deed of trust giving away essentially all his assets and rendering him a pauper. Vitiating factors such as unconscionability and undue influence were considered.11 The question here is whether it would be possible for an artificial intelligence being to exhibit a “conscience”, such that the artificial being may be said to have pressured an individual suffering from an infirmity, to enter into a contract? It is hard to fathom that vitiating factors such as unconscionability would apply to contracts entered into with an artificial intelligence being because existing artificial intelligence technology simply does not exhibit such a “conscience” far removed from human interference in the first place,
7 This is not a far-fetched proposition given the current existence of robo-advisors, see Stella Cramer and Dharmendra Yadav, “Fintech Developments” in Financial Services Law and Regulation (Dora Neo, Hans Tjio and Lan Luh Luh gen eds) (Singapore: Academy Publishing, 2019) ch 16 at para 16.54, p 665.
8 Raj Subramanian, “Living in the world of AI – The Human Transformation” Hackernoon (4 September 2019) <https://hackernoon.com/living-in-the-world-of-ai-the-human-transformation-ve1ds3zyq> (accessed 13 November 2020).
9 See David Fischer et al, “A human brain network derived from coma-causing brainstem lesions” Neurology 2016; 87(23): 2427-2434.
10 BOM v BOK and another appeal [2019] 1 SLR 349 at [102]-[155].
11 See Alexander Loke, “Mistakes in Algorithmic Trading of Cryptocurrencies” (2020) 83 The Modern Law 1343- 1353.
  Electronic copy available at: https://ssrn.com/abstract=3857504

although the importance of an artificial intelligence being possessing consciousness is subject to a number of doctrinal and existential debates.12
Should artificial intelligence objects be granted separate legal personality?
Solaiman observes that the sole purpose for which laws exist is to prevent harm to the community or to confer social benefit, and the law protects these rights by imposing duties on others and providing remedies against any breach thereof.13 Should animate artificial intelligence objects be granted separate legal personality? For instance, an artificial intelligence robot may one day replace human roles. Previously, it was reported that a human being, Akihiko Kondo, had been through a wedding ceremony with a “hologram in a box”, affectionately called Hatsune Miku.14 Even though Miku is no more a virtual entity, Kondo had taken Miku to be the “girl of his dreams who saved him during the lowest point of his life.”15 It is difficult to predict if notions of what it means to be a person will change over time but based on societal norms at present, Miku is obviously not a human being or an entity recognised at law. At which stage should an artificial intelligence system be granted rights and obligations and be ultimately recognised as a person?
A possible way to determine at which point artificial intelligence systems should be granted personhood is perhaps with the assistance of a sci-fi movie. In the 1999 movie, Bicentennial Man, directed by Chris Columbus, it would be possible to categorize Andrew the robot into a sliding scale of consciousness as time progresses.16 At the start, we know that Andrew the robot was built by a company called, NDR, to perform housekeeping and maintenance duty. At this stage, and without being at risk of oversimplification, Andrew the robot could be categorised at the simplest level of automation.17 One still had to charge the robot, turn it on and send it for regular maintenance and the occasional repairs.
As time progressed, it was discovered the robot had a “creative streak”, possibly attributable to its “positronic” brain.18 In today’s parlance, it would be known as deep learning or machine learning.19 At this stage, the CEO of the robotics company commented that this was anomalous and offered to scrap the robot and replace it with a new one. Andrew eventually sought to purchase his independence from his owner, Richard. To cut a long story short, Andrew eventually decided to make contact with
12 Elisabeth Hildt, “Artificial Intelligence: Does Consciousness Matter?” (2019) Front. Psychol. 10:1535.
13 S. M. Solaiman, “Legal personality of robots, corporations, idols and chimpanzees: a quest for legitimacy” (2017) 25 (2) Artificial Intelligence and Law 155-179.
14 Derrick A Paulo, “How AI is coming 'alive', replacing human roles — as robots or as wives” ChannelNewsAsia (6 October 2019) <https://www.channelnewsasia.com/news/cnainsider/how-ai-is-coming-alive-replacing- human-roles-as-robots-or-wives-11973644> (accessed 13 November 2020).
15 Derrick A Paulo, “How AI is coming 'alive', replacing human roles — as robots or as wives” ChannelNewsAsia (6 October 2019) <https://www.channelnewsasia.com/news/cnainsider/how-ai-is-coming-alive-replacing- human-roles-as-robots-or-wives-11973644> (accessed 13 November 2020).
16 Jan Gresil S Kahambing, et al., “Reflecting on the Personality of Artificiality: Reading Asimov’s Film Bicentennial Man through Machine Ethics” (2019) 9(2) Journal of Educational and Social Research 17-24.
17 See Simon Chesterman, “Artificial Intelligence and The Problem of Autonomy”, NUS Law Working Paper Series 2019/016, September 2019, www.law.nus.edu.sg/wps/ at page 8 where he describes the five levels of automation.
18 Isaac Asimov, I, Robot (Oxford University Press, 1993); A positronic brain is a fictional technological device conceived by science fiction writer, Isaac Asimov. According to Isaac Asimov, the positronic brain is the central processing unit for robots and provides them with a form of consciousness recognizable to humans.
19 See Martin Heller, “Deep learning vs machine learning: Understand the differences” InfoWorld (6 January 2020) <https://www.infoworld.com/article/3512245/deep-learning-vs-machine-learning-understand-the- differences.html> (accessed 24 January 2021).
  Electronic copy available at: https://ssrn.com/abstract=3857504

the original NDR robot designer to implant artificial organs20 and he gained the ability to eat, to have sexual relations, and to feel emotions and sensations. At the point when Andrew asked for independence, this was the stage in which Andrew had reached a full consciousness state.21
If the development of artificial intelligence robots reaches this stage sometime in the future, then it becomes highly persuasive to grant them an artificial legal personality, which suggests the ability to hold property, the right to sue and be sued, and the right to work and earn an income and to be paid on time.22
At which point should robots be allowed to incorporate?
The artificial intelligence interface which we have currently (including DeepMind)23 is not at the stage where one can say that it has developed full consciousness (i.e., the ability to feel arousal and awareness) or perhaps to function without any human intervention.24 Nonetheless, technology is evolving rapidly and it is difficult to predict when such a development might occur. At this juncture, it must be noted that the ongoing development of artificial intelligence systems is unprecedented. It should be noted that a former Go champion beaten by DeepMind has retired from professional play after declaring that artificial intelligence is an entity that cannot be defeated.25 Where the artificial intelligence system is at presently is that it makes use of a series of input and output to create new outcomes. This software code has to be created by a natural person, including the hardware to operate the system. In short, one argument is that artificial intelligence in its current form is still very much a machine.26
Perhaps the discourse can best be explained through the case of B2C2 Ltd v Quoine Pte Ltd,27 a Singapore decision which involved the trade of cryptocurrency. The case reflects the downsides from advancements in technology. Yet, the advantages of algorithmic trading cannot be underestimated. It removes the need for a human being to sit at the desk to monitor the market. A computer software does that. The downside is that the end-user of the software could suffer a significant loss when the software malfunctions, especially since actual money is involved in these trades.28 At first instance,
20 This may be possible with bioartificial organ manufacturing technology, see e.g. Wang Xiaohong, “Bioartificial Organ Manufacturing Technologies” (2019) 28(1) Cell Transplant 5-17.
21 Susan Leigh Anderson, “Asimov’s Three Laws of Robotics and Machine Metaethics” (2016) in Science Fiction and Philosophy, Susan Schneider (ed.).
22 S. M. Solaiman, “Legal personality of robots, corporations, idols and chimpanzees: a quest for legitimacy” (2017) 25 (2) Artificial Intelligence and Law 155-179.
23 DeepMind Technologies is an artificial intelligence company. Its program, AlphaGo was able to beat a human professional Go Player Lee Sedol, the world champion, in a five-game match.
24 Ragnar Fjelland, “Why general artificial intelligence will not be realized” (2020) 7 Humanities and Social Sciences Communications 10.
25 James Vincent, “Former Go champion beaten by DeepMind retires after declaring AI invincible” The Verge (27 November 2019) <https://www.theverge.com/2019/11/27/20985260/ai-go-alphago-lee-se-dol-retired- deepmind-defeat> (Accessed 24 January 2020).
26 Thilo Hagendorff and Katharina Wezel, “15 challenges for AI: or what AI (currently) can’t do” (2020) 35 AI & Society 355-365.
27 B2C2 Ltd v Quoine Pte Ltd [2019] 4 SLR 17, on appeal Quoine Pte Ltd v B2C2 Ltd [2020] 2 SLR 20.
28 See Alexander Loke, “Mistakes in Algorithmic Trading of Cryptocurrencies” (2020) 83 The Modern Law 1343- 1353.
  Electronic copy available at: https://ssrn.com/abstract=3857504

the Singapore International Commercial Court (SICC) took the view that it could look at the mindset of a human actor, namely the programmer and his knowledge of the software.29
This presupposes that the trading software programme, while intelligent in every sense of the word, is not deemed as capable of exhibiting its own mind. The algorithmic trading software carried out trades only as they were programmed. Had the software involved deep machine learning, the trading software programme could then “learn” how to trade. Over time, it could develop its own trading algorithm so removed from the original knowledge of the programmer that it becomes both unforeseeable and unfair to ascribe liability on the original programmer, as there will be no relevant human actor’s mindset to consider.
Unless artificial intelligence one day reaches the stage where it is truly an artificial human (and again, that would depend on what the definition to be human truly is at that point in time), it may be argued that we do not need to afford artificial intelligence systems rights. There is simply no need to. The entity liable for any mistakes caused by the artificial intelligence (as a product) should be the manufacturer.30 In certain industry (such as the medical industry), in addition to the fact that liability should fall on the manufacturer to ensure that their product is fit for purpose, perhaps legislation should mandate minimum capital requirements or product liability insurance taken out by the legal entity that the artificial intelligence system is manufactured under in order to be able to meet potential claims for damages.31 The concept of causation and foreseeability would have to be expanded under negligence law,32 for instance, to cover harm caused by artificial intelligence systems.
Hence, on that basis one could advance the proposition that there is no need to grant artificial intelligence systems in its current “weak” form legal personhood. Artificial intelligence systems could be seen as a type of product, such as the personal computer. It is a product to enhance human quality of life, just like what the industrial revolution did for the human race in the 19th century.